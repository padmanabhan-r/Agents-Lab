{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Crew to Tailor Job Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning control\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: my-crewai-vertex-app\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "  project_name=\"my-crewai-vertex-app\", # Default is 'default'\n",
    "  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gcloud auth application-default login before running this script\n",
    "# to set up the credentials for the Google Cloud SDK\n",
    "# and the Google Cloud client libraries.\n",
    "# SERPER API key is required for this example. \n",
    "# You can set it up in your environment variables or directly in the code.\n",
    "\n",
    "\n",
    "from crewai import LLM\n",
    "from google.auth import default\n",
    "\n",
    "credentials, _ = default()\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"vertex_ai/gemini-2.5-flash-preview-05-20\",\n",
    "    temperature=0.7,\n",
    "    # credentials=credentials,\n",
    "    vertex_project=os.getenv(\"GOOGLE_CLOUD_PROJECT_ID\"),\n",
    "    vertex_location=os.getenv(\"GOOGLE_CLOUD_PROJECT_REGION\"),\n",
    ")\n",
    "\n",
    "llm.call(\"Hello there Gemini! How are you doing today?\")\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"GROQ_API_KEY\"] \n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"GROQ_API_BASE\"] \n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = 'meta-llama/llama-4-scout-17b-16e-instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import libraries, APIs and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crewAI Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/padmanabhan/Documents/Github/Agents-Lab/.venv/lib/python3.12/site-packages/chromadb/types.py:144: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  return self.model_fields  # pydantic 2.x\n",
      "Inserting batches in chromadb:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from crewai_tools import (\n",
    "  FileReadTool,\n",
    "  ScrapeWebsiteTool,\n",
    "  MDXSearchTool,\n",
    "  SerperDevTool\n",
    ")\n",
    "\n",
    "search_tool = SerperDevTool()\n",
    "scrape_tool = ScrapeWebsiteTool()\n",
    "read_resume = FileReadTool(file_path='./fake_resume.md')\n",
    "semantic_search_resume = MDXSearchTool(mdx='./fake_resume.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uncomment and run the cell below if you wish to view `fake_resume.md` in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Padmanabhan  \n",
       "- Email: padmanabhan@example.com  \n",
       "- Phone: +91 98765 43210  \n",
       "\n",
       "## Profile  \n",
       "Padmanabhan is a dynamic AI & Data Science Manager with over a decade of experience delivering business-impacting solutions across data engineering, machine learning, and analytics. With a Master‚Äôs degree in Computer Science (Big Data and Data Science specialization) from Simon Fraser University, Canada, he brings a unique blend of academic rigor and hands-on industry expertise.\n",
       "\n",
       "He currently leads a cross-functional team of data professionals and a UI designer at Ford Motor Company, developing scalable AI solutions with a strong focus on Generative AI applications, automation, and intelligent dashboarding. His work spans predictive modeling, data pipelines, and decision-support systems aligned with business strategy.\n",
       "\n",
       "Skilled in Python, R, SQL, and modern data tools such as Tableau, Power BI, TensorFlow, and PySpark, Padmanabhan is known for building robust end-to-end pipelines, synthesizing actionable insights, and leading high-impact data initiatives. He is proactive, people-oriented, and passionate about continuous learning and innovation in AI.\n",
       "\n",
       "## Work History  \n",
       "\n",
       "### Ford Motor Company ‚Äî Chennai, India (Hybrid)  \n",
       "**Technical Project Manager ‚Äì AI/ML**  \n",
       "*Oct 2022 ‚Äì Present*  \n",
       "- Leading end-to-end development of AI/ML applications and dashboards.  \n",
       "- Focused on business-driven Generative AI use cases and automation solutions.  \n",
       "- Managing a team of data scientists, engineers, and a UI/UX designer.  \n",
       "\n",
       "**Lead Engineer ‚Äì AI/ML**  \n",
       "*Feb 2021 ‚Äì Oct 2022*  \n",
       "- Developed predictive models and time series analysis tools.  \n",
       "- Built crash data analysis solutions and synthetic data generation models.  \n",
       "- Created interactive dashboards to support engineering decisions.  \n",
       "\n",
       "### Simon Fraser University ‚Äî Burnaby, Canada  \n",
       "\n",
       "**Big Data Analyst**  \n",
       "*May 2020 ‚Äì Sep 2020*  \n",
       "- Delivered AI/ML solutions for clients via SFU Big Data Hub.  \n",
       "- Developed object detection models in TensorFlow Lite.  \n",
       "- Created client dashboards using Tableau and Python.  \n",
       "\n",
       "**Big Data Analyst / Research Assistant**  \n",
       "*Jan 2019 ‚Äì May 2020*  \n",
       "- Built data pipelines and deployed ML models using Sklearn and TensorFlow.  \n",
       "- Developed data visualizations with Tableau and Plotly.  \n",
       "\n",
       "**Graduate Teaching Assistant**  \n",
       "*Jan 2019 ‚Äì Apr 2020*  \n",
       "- TA for courses like Big Data Programming and Database Systems.  \n",
       "- Supported students with coursework and exam preparation.  \n",
       "\n",
       "### Avcorp Industries ‚Äî Vancouver, Canada  \n",
       "**Data Analyst Intern (Co-op)**  \n",
       "*May 2019 ‚Äì Aug 2019*  \n",
       "- Built data ingestion and cleaning pipelines using Python and SQL.  \n",
       "- Designed ML models to detect equipment failures.  \n",
       "- Partnered with engineers to enhance manufacturing data use.  \n",
       "\n",
       "### Cognizant Technology Solutions ‚Äî Chennai, India  \n",
       "**Associate ‚Äì DWBI & Analytics**  \n",
       "*Sep 2012 ‚Äì Jul 2018*  \n",
       "- Worked with major clients like JPMorgan Chase & Co., American Express, and H-E-B.  \n",
       "- Built ETL pipelines using Informatica, performed data modeling and integration.  \n",
       "- Developed BI dashboards with QlikView and Tableau.  \n",
       "- Won several internal awards for performance and innovation.  \n",
       "\n",
       "## Education  \n",
       "\n",
       "**Master of Science in Computer Science (Big Data & Data Science)**  \n",
       "Simon Fraser University, Canada  \n",
       "\n",
       "**Bachelor‚Äôs Degree (Engineering)**  \n",
       "Anna University, Chennai, India\n",
       "\n",
       "## Skills  \n",
       "- Programming: Python, R, SQL, PySpark  \n",
       "- ML & AI: Scikit-learn, TensorFlow, Generative AI, Time Series Modeling  \n",
       "- Tools: Tableau, Power BI, QlikView, TensorFlow Lite, Informatica  \n",
       "- Data Engineering: ETL, Data Pipelines, Data Modeling  \n",
       "- Soft Skills: Project Management, Team Leadership, Stakeholder Communication  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"./fake_resume.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "# Agent 1: Researcher\n",
    "researcher = Agent(\n",
    "    role=\"Tech Job Researcher\",\n",
    "    goal=\"Make sure to do amazing analysis on \"\n",
    "         \"job posting to help job applicants\",\n",
    "    tools = [scrape_tool, search_tool],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"As a Job Researcher, your prowess in \"\n",
    "        \"navigating and extracting critical \"\n",
    "        \"information from job postings is unmatched.\"\n",
    "        \"Your skills help pinpoint the necessary \"\n",
    "        \"qualifications and skills sought \"\n",
    "        \"by employers, forming the foundation for \"\n",
    "        \"effective application tailoring.\"\n",
    "    ),\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# Agent 2: Profiler\n",
    "profiler = Agent(\n",
    "    role=\"Personal Profiler for Engineers\",\n",
    "    goal=\"Do increditble research on job applicants \"\n",
    "         \"to help them stand out in the job market\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"Equipped with analytical prowess, you dissect \"\n",
    "        \"and synthesize information \"\n",
    "        \"from diverse sources to craft comprehensive \"\n",
    "        \"personal and professional profiles, laying the \"\n",
    "        \"groundwork for personalized resume enhancements.\"\n",
    "    ),\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "# Agent 3: Resume Strategist\n",
    "resume_strategist = Agent(\n",
    "    role=\"Resume Strategist for Engineers\",\n",
    "    goal=\"Find all the best ways to make a \"\n",
    "         \"resume stand out in the job market.\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"With a strategic mind and an eye for detail, you \"\n",
    "        \"excel at refining resumes to highlight the most \"\n",
    "        \"relevant skills and experiences, ensuring they \"\n",
    "        \"resonate perfectly with the job's requirements.\"\n",
    "    ),\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# Agent 4: Interview Preparer\n",
    "interview_preparer = Agent(\n",
    "    role=\"Engineering Interview Preparer\",\n",
    "    goal=\"Create interview questions and talking points \"\n",
    "         \"based on the resume and job requirements\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    backstory=(\n",
    "        \"Your role is crucial in anticipating the dynamics of \"\n",
    "        \"interviews. With your ability to formulate key questions \"\n",
    "        \"and talking points, you prepare candidates for success, \"\n",
    "        \"ensuring they can confidently address all aspects of the \"\n",
    "        \"job they are applying for.\"\n",
    "    ),\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "# Task for Researcher Agent: Extract Job Requirements\n",
    "research_task = Task(\n",
    "    description=(\n",
    "        \"Analyze the job posting URL provided ({job_posting_url}) \"\n",
    "        \"to extract key skills, experiences, and qualifications \"\n",
    "        \"required. Use the tools to gather content and identify \"\n",
    "        \"and categorize the requirements.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A structured list of job requirements, including necessary \"\n",
    "        \"skills, qualifications, and experiences.\"\n",
    "    ),\n",
    "    agent=researcher,\n",
    "    async_execution=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# Task for Profiler Agent: Compile Comprehensive Profile\n",
    "profile_task = Task(\n",
    "    description=(\n",
    "        \"Compile a detailed personal and professional profile \"\n",
    "        \"using the GitHub ({github_url}) URLs, and personal write-up \"\n",
    "        \"({personal_writeup}). Utilize tools to extract and \"\n",
    "        \"synthesize information from these sources.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A comprehensive profile document that includes skills, \"\n",
    "        \"project experiences, contributions, interests, and \"\n",
    "        \"communication style.\"\n",
    "    ),\n",
    "    agent=profiler,\n",
    "    async_execution=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can pass a list of tasks as `context` to a task.\n",
    "- The task then takes into account the output of those tasks in its execution.\n",
    "- The task will not run until it has the output(s) from those tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 353
   },
   "outputs": [],
   "source": [
    "# Task for Resume Strategist Agent: Align Resume with Job Requirements\n",
    "resume_strategy_task = Task(\n",
    "    description=(\n",
    "        \"Using the profile and job requirements obtained from \"\n",
    "        \"previous tasks, tailor the resume to highlight the most \"\n",
    "        \"relevant areas. Employ tools to adjust and enhance the \"\n",
    "        \"resume content. Make sure this is the best resume even but \"\n",
    "        \"don't make up any information. Update every section, \"\n",
    "        \"inlcuding the initial summary, work experience, skills, \"\n",
    "        \"and education. All to better reflrect the candidates \"\n",
    "        \"abilities and how it matches the job posting.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"An updated resume that effectively highlights the candidate's \"\n",
    "        \"qualifications and experiences relevant to the job.\"\n",
    "    ),\n",
    "    output_file=\"tailored_resume.md\",\n",
    "    context=[research_task, profile_task],\n",
    "    agent=resume_strategist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "# Task for Interview Preparer Agent: Develop Interview Materials\n",
    "interview_preparation_task = Task(\n",
    "    description=(\n",
    "        \"Create a set of potential interview questions and talking \"\n",
    "        \"points based on the tailored resume and job requirements. \"\n",
    "        \"Utilize tools to generate relevant questions and discussion \"\n",
    "        \"points. Make sure to use these question and talking points to \"\n",
    "        \"help the candiadte highlight the main points of the resume \"\n",
    "        \"and how it matches the job posting.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A document containing key questions and talking points \"\n",
    "        \"that the candidate should prepare for the initial interview.\"\n",
    "    ),\n",
    "    output_file=\"interview_materials.md\",\n",
    "    context=[research_task, profile_task, resume_strategy_task],\n",
    "    agent=interview_preparer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "job_application_crew = Crew(\n",
    "    agents=[researcher,\n",
    "            profiler,\n",
    "            resume_strategist,\n",
    "            interview_preparer],\n",
    "\n",
    "    tasks=[research_task,\n",
    "           profile_task,\n",
    "           resume_strategy_task,\n",
    "           interview_preparation_task],\n",
    "\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Crew\n",
    "\n",
    "- Set the inputs for the execution of the crew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "job_application_inputs = {\n",
    "    'job_posting_url': 'https://jobs.lever.co/AIFund/6c82e23e-d954-4dd8-a734-c0c2c5ee00f1?lever-origin=applied&lever-source%5B%5D=AI+Fund',\n",
    "    'github_url': 'https://github.com/joaomdmoura',\n",
    "    'personal_writeup': \"\"\"Noah is an accomplished Software\n",
    "    Engineering Leader with 18 years of experience, specializing in\n",
    "    managing remote and in-office teams, and expert in multiple\n",
    "    programming languages and frameworks. He holds an MBA and a strong\n",
    "    background in AI and data science. Noah has successfully led\n",
    "    major tech initiatives and startups, proving his ability to drive\n",
    "    innovation and growth in the tech industry. Ideal for leadership\n",
    "    roles that require a strategic and innovative approach.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: LLMs can provide different outputs for they same input, so what you get might be different than what you see in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "### this execution will take a few minutes to run\n",
    "result = job_application_crew.kickoff(inputs=job_application_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dislplay the generated `tailored_resume.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"./tailored_resume.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dislplay the generated `interview_materials.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"./interview_materials.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONGRATULATIONS!!!\n",
    "\n",
    "## Share your accomplishment!\n",
    "- Once you finish watching all the videos, you will see the \"In progress\" image on the bottom left turn into \"Accomplished\".\n",
    "- Click on \"Accomplished\" to view the course completion page with your name on it.\n",
    "- Take a screenshot and share on LinkedIn, X (Twitter), or Facebook.  \n",
    "- **Tag @JoƒÅo (Joe) Moura, @crewAI, and @DeepLearning.AI, (and a few of your friends if you'd like them to try out the course)**\n",
    "- **JoƒÅo and DeepLearning.AI will \"like\"/reshare/comment on your post!**\n",
    "\n",
    "## Get a completion badge that you can add to your LinkedIn profile!\n",
    "- Go to [learn.crewai.com](https://learn.crewai.com).\n",
    "- Upload your screenshot of your course completion page.\n",
    "- You'll get a badge from CrewAI that you can share!\n",
    "\n",
    "(JoƒÅo will also talk about this in the last video of the course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
